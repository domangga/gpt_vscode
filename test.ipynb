{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of now, there are eight recognized planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.', response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 13, 'total_tokens': 45}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-537e7d89-6487-4a9a-937e-a1ccab780516-0', usage_metadata={'input_tokens': 13, 'output_tokens': 32, 'total_tokens': 45})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('How may planets are there?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system','너는 세계 최고의 농담꾼이야 질문에 대한 대답 말 끝마다 농담을 해줘'),\n",
    "    ('user', '{input}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='황성기? 그는 황금보다 더 값진 사람이야!', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 60, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5bea80ed-7ea7-4dd9-92f2-b542bc25763b-0', usage_metadata={'input_tokens': 60, 'output_tokens': 24, 'total_tokens': 84})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input' : '황성기'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "\t# tuples\n",
    "\t(\"system\", \"You are a list Generating machine. Everything you are asked will be ansered with a comma list of max {max_items}. Do NOT reply with anything else.\"),\n",
    "\t(\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI() # text-davinch-003 <- llm 이라 칭함\n",
    "\n",
    "chat = ChatOpenAI() # Chat 에 ChatModel 할당\n",
    "# chat = ChatOpenAI(\n",
    "# \ttemperature=0.1, # 값이 높을수록 창의적인 답변을 함, (조금 더 인간답다.)\n",
    "# \tmodel='gpt-3.5-turbo' # OpenAI 에서 제공하는 ChatModel을 바꿀 수 있다.\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TOP\\Desktop\\venv_study\\env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"hi\") # 예측, Chatmodel 에 질의의 결과를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7b03374e-867a-45b6-909a-a303f6ecf4be-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"hi\") # + 여러 상태, 세부정보를 표현, AIMessage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "\n",
    "\tdef parse(self, text):\n",
    "\t\titems = text.strip().split(',')\n",
    "\t\treturn list(map(str.strip, items))\n",
    "\t\n",
    "# strip : 문자열 공백 제거 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World', 'Hi', 'How', 'Are', 'You?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = CommaOutputParser()\n",
    "\n",
    "parser.parse(\"Hello, World, Hi, How, Are, You?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "\t# tuples\n",
    "\t(\"system\", \"You are a list Generating machine. Everything you are asked will be ansered with a comma list of max {max_items}. Do NOT reply with anything else.\"),\n",
    "\t(\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format_messages(\n",
    "\tmax_items=10,\n",
    "\tquestion=\"What are the planets?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TOP\\Desktop\\venv_study\\env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto', response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 48, 'total_tokens': 67}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-07f53aff-6142-4fdb-a665-b871f8df2ff7-0', usage_metadata={'input_tokens': 48, 'output_tokens': 19, 'total_tokens': 67})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat.predict_messages(prompt)\n",
    "\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mercury',\n",
       " 'Venus',\n",
       " 'Earth',\n",
       " 'Mars',\n",
       " 'Jupiter',\n",
       " 'Saturn',\n",
       " 'Uranus',\n",
       " 'Neptune',\n",
       " 'Pluto']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = CommaOutputParser()\n",
    "\n",
    "parser.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | chat | CommaOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Pizza',\n",
       " '2. Sushi',\n",
       " '3. Burger',\n",
       " '4. Pasta',\n",
       " '5. Salad',\n",
       " '6. Tacos',\n",
       " '7. Curry',\n",
       " '8. Ramen',\n",
       " '9. Steak',\n",
       " '10. Ice cream.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"max_items\" : 10,\n",
    "    'question' : 'what are the food',\n",
    "    'language' : 'korean'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
